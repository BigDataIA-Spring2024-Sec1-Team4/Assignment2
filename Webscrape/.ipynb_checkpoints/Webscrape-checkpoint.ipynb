{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7757843d",
   "metadata": {},
   "source": [
    "### Installing Requirements and importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f71683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.17.2-py3-none-any.whl (9.9 MB)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (4.11.1)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Using cached trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from selenium->-r requirements.txt (line 1)) (1.26.11)\n",
      "Collecting trio~=0.17\n",
      "  Using cached trio-0.24.0-py3-none-any.whl (460 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from selenium->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from selenium->-r requirements.txt (line 1)) (2022.6.15)\n",
      "Requirement already satisfied: packaging in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: python-dotenv in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager->-r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from requests->-r requirements.txt (line 3)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from requests->-r requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->-r requirements.txt (line 4)) (2.3.1)\n",
      "Collecting sniffio>=1.3.0\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting outcome\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: exceptiongroup in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (21.4.0)\n",
      "Collecting wsproto>=0.14\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/anirudhajoshi/opt/anaconda3/lib/python3.9/site-packages (from packaging->webdriver-manager->-r requirements.txt (line 2)) (3.0.9)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sniffio, outcome, h11, wsproto, webdriver-manager, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.17.2 sniffio-1.3.0 trio-0.24.0 trio-websocket-0.11.1 webdriver-manager-4.0.1 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82daac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a05d48",
   "metadata": {},
   "source": [
    "Scraping the 224 individual refresher readings links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf4ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "total=0\n",
    "page_link = []\n",
    "\n",
    "def scrape_links(driver):\n",
    "    n=0\n",
    "    global total\n",
    "    global page_link\n",
    "    links = driver.find_elements(By.CSS_SELECTOR, 'div.cfa-coveo a')\n",
    "    for link in links:\n",
    "        href = link.get_attribute('href')\n",
    "        if href is not None:\n",
    "            n=n+1\n",
    "            page_link.append(href)\n",
    "            print(href)\n",
    "        \n",
    "    total=total+n\n",
    "    \n",
    "def go_to_next_page(driver):\n",
    "    try:\n",
    "        next_button = driver.find_element(By.CSS_SELECTOR, \"li.coveo-pager-next\")\n",
    "        \n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        return True\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "\n",
    "url = 'https://www.cfainstitute.org/en/membership/professional-development/refresher-readings'\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, 600).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.cfa-coveo')))\n",
    "        scrape_links(driver)\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break  \n",
    "    if not go_to_next_page(driver):\n",
    "        print(\"No more pages or next button not found.\")\n",
    "        break\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "urls = page_link.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0527246",
   "metadata": {},
   "source": [
    "Extracting the data individual 224 webpages and storing it CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3226fb39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'page_link' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dj/wcp4hvwd7wzglrm85gjck8lr0000gn/T/ipykernel_65087/959773094.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage_link\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcsv_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'extracted.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# CSV headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'page_link' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "csv_file_path = 'extracted.csv'\n",
    "\n",
    "# CSV headers\n",
    "headers = ['Name_of_the_topic', 'Year', 'Level', 'Introduction_Summary', 'Learning_Outcomes', 'Link_to_the_Summary_Page', 'Link_to_the_PDF_File']\n",
    "\n",
    "\n",
    "def safe_extract_text(soup, selector, attribute=None):\n",
    "    element = soup.select_one(selector)\n",
    "    if attribute:\n",
    "        return element[attribute] if element and attribute in element.attrs else \"None\"\n",
    "    return element.get_text(strip=True) if element else \"None\"\n",
    "\n",
    "def extract_following_text(header):\n",
    "    if header:\n",
    "        content = []\n",
    "        for sibling in header.find_next_siblings():\n",
    "            if sibling.name == \"h2\":\n",
    "                break  # Stop if we encounter another header\n",
    "            content.append(sibling.get_text(strip=True))\n",
    "        return \" \".join(content)\n",
    "    return \"None\"\n",
    "\n",
    "with open(csv_file_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for url in urls:\n",
    "        # Fetch the HTML content of the page\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        \n",
    "        curriculum_year = safe_extract_text(soup, \".content-utility-curriculum\")\n",
    "        level = safe_extract_text(soup, \".content-utility-level\")\n",
    "        topic = safe_extract_text(soup, \".content-utility-topic\")\n",
    "        pdf_link = safe_extract_text(soup, \"a.locked-content\", \"href\")\n",
    "        \n",
    "        \n",
    "        introduction_header = soup.find(\"h2\", text=\"Introduction\")\n",
    "        introduction_text = extract_following_text(introduction_header)\n",
    "        \n",
    "        learning_outcomes_header = soup.find(\"h2\", text=\"Learning Outcomes\")\n",
    "        learning_outcomes_text = extract_following_text(learning_outcomes_header)\n",
    "        \n",
    "        # Write the extracted data to the CSV\n",
    "        data = {\n",
    "            \"Name_of_the_topic\": topic,\n",
    "            \"Year\": curriculum_year,\n",
    "            \"Level\": level,\n",
    "            \"Introduction_Summary\": introduction_text,\n",
    "            \"Learning_Outcomes\": learning_outcomes_text,\n",
    "            \"Link_to_the_Summary_Page\": url,\n",
    "            \"Link_to_the_PDF_File\": pdf_link\n",
    "        }\n",
    "        \n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1093f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
